{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27037943",
   "metadata": {},
   "source": [
    "# Тема “Обучение с учителем”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c0a131",
   "metadata": {},
   "source": [
    "Задание 1\n",
    "Импортируйте библиотеки pandas и numpy.\n",
    "Загрузите \"Boston House Prices dataset\" из встроенных наборов данных библиотеки sklearn. Создайте датафреймы X и y из этих данных.\n",
    "Разбейте эти датафреймы на тренировочные (X_train, y_train) и тестовые (X_test, y_test) с помощью функции train_test_split так, чтобы размер тестовой выборки\n",
    "составлял 30% от всех данных, при этом аргумент random_state должен быть равен 42.\n",
    "Создайте модель линейной регрессии под названием lr с помощью класса LinearRegression из модуля sklearn.linear_model.\n",
    "Обучите модель на тренировочных данных (используйте все признаки) и сделайте предсказание на тестовых.\n",
    "Вычислите R2 полученных предказаний с помощью r2_score из модуля sklearn.metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b2959f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d7b6ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "614ed49a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boston = load_boston()\n",
    "boston.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc078830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(506, 13)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = boston[\"data\"]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "657a4d65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD',\n",
       "       'TAX', 'PTRATIO', 'B', 'LSTAT'], dtype='<U7')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = boston[\"feature_names\"]\n",
    "feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf809e2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24. , 21.6, 34.7, 33.4, 36.2, 28.7, 22.9, 27.1, 16.5, 18.9])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = boston[\"target\"]\n",
    "target[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb68b13a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2.0</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3.0</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD    TAX  \\\n",
       "0  0.00632  18.0   2.31   0.0  0.538  6.575  65.2  4.0900  1.0  296.0   \n",
       "1  0.02731   0.0   7.07   0.0  0.469  6.421  78.9  4.9671  2.0  242.0   \n",
       "2  0.02729   0.0   7.07   0.0  0.469  7.185  61.1  4.9671  2.0  242.0   \n",
       "3  0.03237   0.0   2.18   0.0  0.458  6.998  45.8  6.0622  3.0  222.0   \n",
       "4  0.06905   0.0   2.18   0.0  0.458  7.147  54.2  6.0622  3.0  222.0   \n",
       "\n",
       "   PTRATIO       B  LSTAT  \n",
       "0     15.3  396.90   4.98  \n",
       "1     17.8  396.90   9.14  \n",
       "2     17.8  392.83   4.03  \n",
       "3     18.7  394.63   2.94  \n",
       "4     18.7  396.90   5.33  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.DataFrame(data, columns=feature_names)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0e67aa39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 13 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   CRIM     506 non-null    float64\n",
      " 1   ZN       506 non-null    float64\n",
      " 2   INDUS    506 non-null    float64\n",
      " 3   CHAS     506 non-null    float64\n",
      " 4   NOX      506 non-null    float64\n",
      " 5   RM       506 non-null    float64\n",
      " 6   AGE      506 non-null    float64\n",
      " 7   DIS      506 non-null    float64\n",
      " 8   RAD      506 non-null    float64\n",
      " 9   TAX      506 non-null    float64\n",
      " 10  PTRATIO  506 non-null    float64\n",
      " 11  B        506 non-null    float64\n",
      " 12  LSTAT    506 non-null    float64\n",
      "dtypes: float64(13)\n",
      "memory usage: 51.5 KB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8466993f",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = boston[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80ee89ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 1 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   price   506 non-null    float64\n",
      "dtypes: float64(1)\n",
      "memory usage: 4.1 KB\n"
     ]
    }
   ],
   "source": [
    "y = pd.DataFrame(target, columns=[\"price\"])\n",
    "\n",
    "y.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd06f8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb1d21d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e10d5798",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c07296f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec9888e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c6a37fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e8e5ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>23.6</td>\n",
       "      <td>28.648960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>32.4</td>\n",
       "      <td>36.495014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>13.6</td>\n",
       "      <td>15.411193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>22.8</td>\n",
       "      <td>25.403213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>16.1</td>\n",
       "      <td>18.855280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>20.0</td>\n",
       "      <td>23.146689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>17.8</td>\n",
       "      <td>17.392124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>14.0</td>\n",
       "      <td>14.078599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>19.6</td>\n",
       "      <td>23.036927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>16.8</td>\n",
       "      <td>20.599433</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     y_test     y_pred\n",
       "173    23.6  28.648960\n",
       "274    32.4  36.495014\n",
       "491    13.6  15.411193\n",
       "72     22.8  25.403213\n",
       "452    16.1  18.855280\n",
       "76     20.0  23.146689\n",
       "316    17.8  17.392124\n",
       "140    14.0  14.078599\n",
       "471    19.6  23.036927\n",
       "500    16.8  20.599433"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_test = pd.DataFrame({\n",
    "    \"y_test\": y_test[\"price\"],\n",
    "    \"y_pred\": y_pred.flatten(),\n",
    "})\n",
    "\n",
    "check_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e5186cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e08e84e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6693702691495587"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2 = r2_score(check_test['y_pred'], check_test['y_test'])\n",
    "r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902541ee",
   "metadata": {},
   "source": [
    "Задание 2\n",
    "Создайте модель под названием model с помощью RandomForestRegressor из модуля sklearn.ensemble.\n",
    "Сделайте агрумент n_estimators равным 1000,\n",
    "max_depth должен быть равен 12 и random_state сделайте равным 42.\n",
    "Обучите модель на тренировочных данных аналогично тому, как вы обучали модель LinearRegression,\n",
    "но при этом в метод fit вместо датафрейма y_train поставьте y_train.values[:, 0],\n",
    "чтобы получить из датафрейма одномерный массив Numpy,\n",
    "так как для класса RandomForestRegressor в данном методе для аргумента y предпочтительно применение массивов вместо датафрейма.\n",
    "Сделайте предсказание на тестовых данных и посчитайте R2. Сравните с результатом из предыдущего задания.\n",
    "Напишите в комментариях к коду, какая модель в данном случае работает лучше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "676e7b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba68d77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(n_estimators = 1000, max_depth=12, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "440bce8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=12, n_estimators=1000, random_state=42)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train.values[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7c39c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "02302c09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y_test</th>\n",
       "      <th>y_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>23.6</td>\n",
       "      <td>22.806412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>32.4</td>\n",
       "      <td>31.131464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>491</th>\n",
       "      <td>13.6</td>\n",
       "      <td>16.339125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>22.8</td>\n",
       "      <td>23.810726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>452</th>\n",
       "      <td>16.1</td>\n",
       "      <td>17.139521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>20.0</td>\n",
       "      <td>21.832284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>17.8</td>\n",
       "      <td>19.895747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>14.0</td>\n",
       "      <td>14.754118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>19.6</td>\n",
       "      <td>21.240835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>16.8</td>\n",
       "      <td>20.898658</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     y_test     y_pred\n",
       "173    23.6  22.806412\n",
       "274    32.4  31.131464\n",
       "491    13.6  16.339125\n",
       "72     22.8  23.810726\n",
       "452    16.1  17.139521\n",
       "76     20.0  21.832284\n",
       "316    17.8  19.895747\n",
       "140    14.0  14.754118\n",
       "471    19.6  21.240835\n",
       "500    16.8  20.898658"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_test = pd.DataFrame({\n",
    "    \"y_test\": y_test[\"price\"],\n",
    "    \"y_pred\": y_pred.flatten(),\n",
    "})\n",
    "\n",
    "check_test.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb1a185a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8479049999699443"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2 = r2_score(check_test['y_pred'], check_test['y_test'])\n",
    "r2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb774f28",
   "metadata": {},
   "source": [
    "RandomForestRegressor работает лучше чем LinearRegression, т.к. значение R2 модели RandomForestRegressor ближе к единице."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f38b633",
   "metadata": {},
   "source": [
    "*Задание 3\n",
    "Вызовите документацию для класса RandomForestRegressor,\n",
    "найдите информацию об атрибуте feature_importances_.\n",
    "С помощью этого атрибута найдите сумму всех показателей важности,\n",
    "установите, какие два признака показывают наибольшую важность."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "264956eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " RandomForestRegressor(n_estimators=100, *, criterion='mse', max_depth=None,\n",
      "                       min_samples_split=2, min_samples_leaf=1,\n",
      "                       min_weight_fraction_leaf=0.0, max_features='auto',\n",
      "                       max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                       min_impurity_split=None, bootstrap=True,\n",
      "                       oob_score=False, n_jobs=None, random_state=None,\n",
      "                       verbose=0, warm_start=False, ccp_alpha=0.0,\n",
      "                       max_samples=None)\n",
      "\n",
      "A random forest regressor.\n",
      "\n",
      "A random forest is a meta estimator that fits a number of classifying\n",
      "decision trees on various sub-samples of the dataset and uses averaging\n",
      "to improve the predictive accuracy and control over-fitting.\n",
      "The sub-sample size is controlled with the `max_samples` parameter if\n",
      "`bootstrap=True` (default), otherwise the whole dataset is used to build\n",
      "each tree.\n",
      "\n",
      "Read more in the :ref:`User Guide <forest>`.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "n_estimators : int, default=100\n",
      "    The number of trees in the forest.\n",
      "\n",
      "    .. versionchanged:: 0.22\n",
      "       The default value of ``n_estimators`` changed from 10 to 100\n",
      "       in 0.22.\n",
      "\n",
      "criterion : {\"mse\", \"mae\"}, default=\"mse\"\n",
      "    The function to measure the quality of a split. Supported criteria\n",
      "    are \"mse\" for the mean squared error, which is equal to variance\n",
      "    reduction as feature selection criterion, and \"mae\" for the mean\n",
      "    absolute error.\n",
      "\n",
      "    .. versionadded:: 0.18\n",
      "       Mean Absolute Error (MAE) criterion.\n",
      "\n",
      "max_depth : int, default=None\n",
      "    The maximum depth of the tree. If None, then nodes are expanded until\n",
      "    all leaves are pure or until all leaves contain less than\n",
      "    min_samples_split samples.\n",
      "\n",
      "min_samples_split : int or float, default=2\n",
      "    The minimum number of samples required to split an internal node:\n",
      "\n",
      "    - If int, then consider `min_samples_split` as the minimum number.\n",
      "    - If float, then `min_samples_split` is a fraction and\n",
      "      `ceil(min_samples_split * n_samples)` are the minimum\n",
      "      number of samples for each split.\n",
      "\n",
      "    .. versionchanged:: 0.18\n",
      "       Added float values for fractions.\n",
      "\n",
      "min_samples_leaf : int or float, default=1\n",
      "    The minimum number of samples required to be at a leaf node.\n",
      "    A split point at any depth will only be considered if it leaves at\n",
      "    least ``min_samples_leaf`` training samples in each of the left and\n",
      "    right branches.  This may have the effect of smoothing the model,\n",
      "    especially in regression.\n",
      "\n",
      "    - If int, then consider `min_samples_leaf` as the minimum number.\n",
      "    - If float, then `min_samples_leaf` is a fraction and\n",
      "      `ceil(min_samples_leaf * n_samples)` are the minimum\n",
      "      number of samples for each node.\n",
      "\n",
      "    .. versionchanged:: 0.18\n",
      "       Added float values for fractions.\n",
      "\n",
      "min_weight_fraction_leaf : float, default=0.0\n",
      "    The minimum weighted fraction of the sum total of weights (of all\n",
      "    the input samples) required to be at a leaf node. Samples have\n",
      "    equal weight when sample_weight is not provided.\n",
      "\n",
      "max_features : {\"auto\", \"sqrt\", \"log2\"}, int or float, default=\"auto\"\n",
      "    The number of features to consider when looking for the best split:\n",
      "\n",
      "    - If int, then consider `max_features` features at each split.\n",
      "    - If float, then `max_features` is a fraction and\n",
      "      `round(max_features * n_features)` features are considered at each\n",
      "      split.\n",
      "    - If \"auto\", then `max_features=n_features`.\n",
      "    - If \"sqrt\", then `max_features=sqrt(n_features)`.\n",
      "    - If \"log2\", then `max_features=log2(n_features)`.\n",
      "    - If None, then `max_features=n_features`.\n",
      "\n",
      "    Note: the search for a split does not stop until at least one\n",
      "    valid partition of the node samples is found, even if it requires to\n",
      "    effectively inspect more than ``max_features`` features.\n",
      "\n",
      "max_leaf_nodes : int, default=None\n",
      "    Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
      "    Best nodes are defined as relative reduction in impurity.\n",
      "    If None then unlimited number of leaf nodes.\n",
      "\n",
      "min_impurity_decrease : float, default=0.0\n",
      "    A node will be split if this split induces a decrease of the impurity\n",
      "    greater than or equal to this value.\n",
      "\n",
      "    The weighted impurity decrease equation is the following::\n",
      "\n",
      "        N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
      "                            - N_t_L / N_t * left_impurity)\n",
      "\n",
      "    where ``N`` is the total number of samples, ``N_t`` is the number of\n",
      "    samples at the current node, ``N_t_L`` is the number of samples in the\n",
      "    left child, and ``N_t_R`` is the number of samples in the right child.\n",
      "\n",
      "    ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
      "    if ``sample_weight`` is passed.\n",
      "\n",
      "    .. versionadded:: 0.19\n",
      "\n",
      "min_impurity_split : float, default=None\n",
      "    Threshold for early stopping in tree growth. A node will split\n",
      "    if its impurity is above the threshold, otherwise it is a leaf.\n",
      "\n",
      "    .. deprecated:: 0.19\n",
      "       ``min_impurity_split`` has been deprecated in favor of\n",
      "       ``min_impurity_decrease`` in 0.19. The default value of\n",
      "       ``min_impurity_split`` has changed from 1e-7 to 0 in 0.23 and it\n",
      "       will be removed in 1.0 (renaming of 0.25).\n",
      "       Use ``min_impurity_decrease`` instead.\n",
      "\n",
      "bootstrap : bool, default=True\n",
      "    Whether bootstrap samples are used when building trees. If False, the\n",
      "    whole dataset is used to build each tree.\n",
      "\n",
      "oob_score : bool, default=False\n",
      "    whether to use out-of-bag samples to estimate\n",
      "    the R^2 on unseen data.\n",
      "\n",
      "n_jobs : int, default=None\n",
      "    The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
      "    :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
      "    trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
      "    context. ``-1`` means using all processors. See :term:`Glossary\n",
      "    <n_jobs>` for more details.\n",
      "\n",
      "random_state : int, RandomState instance or None, default=None\n",
      "    Controls both the randomness of the bootstrapping of the samples used\n",
      "    when building trees (if ``bootstrap=True``) and the sampling of the\n",
      "    features to consider when looking for the best split at each node\n",
      "    (if ``max_features < n_features``).\n",
      "    See :term:`Glossary <random_state>` for details.\n",
      "\n",
      "verbose : int, default=0\n",
      "    Controls the verbosity when fitting and predicting.\n",
      "\n",
      "warm_start : bool, default=False\n",
      "    When set to ``True``, reuse the solution of the previous call to fit\n",
      "    and add more estimators to the ensemble, otherwise, just fit a whole\n",
      "    new forest. See :term:`the Glossary <warm_start>`.\n",
      "\n",
      "ccp_alpha : non-negative float, default=0.0\n",
      "    Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
      "    subtree with the largest cost complexity that is smaller than\n",
      "    ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
      "    :ref:`minimal_cost_complexity_pruning` for details.\n",
      "\n",
      "    .. versionadded:: 0.22\n",
      "\n",
      "max_samples : int or float, default=None\n",
      "    If bootstrap is True, the number of samples to draw from X\n",
      "    to train each base estimator.\n",
      "\n",
      "    - If None (default), then draw `X.shape[0]` samples.\n",
      "    - If int, then draw `max_samples` samples.\n",
      "    - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n",
      "      `max_samples` should be in the interval `(0, 1)`.\n",
      "\n",
      "    .. versionadded:: 0.22\n",
      "\n",
      "Attributes\n",
      "----------\n",
      "base_estimator_ : DecisionTreeRegressor\n",
      "    The child estimator template used to create the collection of fitted\n",
      "    sub-estimators.\n",
      "\n",
      "estimators_ : list of DecisionTreeRegressor\n",
      "    The collection of fitted sub-estimators.\n",
      "\n",
      "feature_importances_ : ndarray of shape (n_features,)\n",
      "    The impurity-based feature importances.\n",
      "    The higher, the more important the feature.\n",
      "    The importance of a feature is computed as the (normalized)\n",
      "    total reduction of the criterion brought by that feature.  It is also\n",
      "    known as the Gini importance.\n",
      "\n",
      "    Warning: impurity-based feature importances can be misleading for\n",
      "    high cardinality features (many unique values). See\n",
      "    :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
      "\n",
      "n_features_ : int\n",
      "    The number of features when ``fit`` is performed.\n",
      "\n",
      "n_outputs_ : int\n",
      "    The number of outputs when ``fit`` is performed.\n",
      "\n",
      "oob_score_ : float\n",
      "    Score of the training dataset obtained using an out-of-bag estimate.\n",
      "    This attribute exists only when ``oob_score`` is True.\n",
      "\n",
      "oob_prediction_ : ndarray of shape (n_samples,)\n",
      "    Prediction computed with out-of-bag estimate on the training set.\n",
      "    This attribute exists only when ``oob_score`` is True.\n",
      "\n",
      "See Also\n",
      "--------\n",
      "DecisionTreeRegressor, ExtraTreesRegressor\n",
      "\n",
      "Notes\n",
      "-----\n",
      "The default values for the parameters controlling the size of the trees\n",
      "(e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
      "unpruned trees which can potentially be very large on some data sets. To\n",
      "reduce memory consumption, the complexity and size of the trees should be\n",
      "controlled by setting those parameter values.\n",
      "\n",
      "The features are always randomly permuted at each split. Therefore,\n",
      "the best found split may vary, even with the same training data,\n",
      "``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
      "of the criterion is identical for several splits enumerated during the\n",
      "search of the best split. To obtain a deterministic behaviour during\n",
      "fitting, ``random_state`` has to be fixed.\n",
      "\n",
      "The default value ``max_features=\"auto\"`` uses ``n_features``\n",
      "rather than ``n_features / 3``. The latter was originally suggested in\n",
      "[1], whereas the former was more recently justified empirically in [2].\n",
      "\n",
      "References\n",
      "----------\n",
      ".. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
      "\n",
      ".. [2] P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized\n",
      "       trees\", Machine Learning, 63(1), 3-42, 2006.\n",
      "\n",
      "Examples\n",
      "--------\n",
      ">>> from sklearn.ensemble import RandomForestRegressor\n",
      ">>> from sklearn.datasets import make_regression\n",
      ">>> X, y = make_regression(n_features=4, n_informative=2,\n",
      "...                        random_state=0, shuffle=False)\n",
      ">>> regr = RandomForestRegressor(max_depth=2, random_state=0)\n",
      ">>> regr.fit(X, y)\n",
      "RandomForestRegressor(...)\n",
      ">>> print(regr.predict([[0, 0, 0, 0]]))\n",
      "[-8.32987858]\n",
      "\n",
      "\n",
      "Methods:\n",
      "\n",
      "  apply  --  Apply trees in the forest to X, return leaf indices.\n",
      "  decision_path  --  Return the decision path in the forest.\n",
      "  fit  --  Build a forest of trees from the training set (X, y).\n",
      "  get_params  --  Get parameters for this estimator.\n",
      "  predict  --  Predict regression target for X.\n",
      "  score  --  \n",
      "  set_params  --  Set the parameters of this estimator.\n"
     ]
    }
   ],
   "source": [
    "np.info(RandomForestRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e7efe71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03167574 0.00154252 0.00713813 0.00123624 0.01426897 0.40268179\n",
      " 0.01429864 0.06397257 0.00528122 0.01152493 0.01808108 0.01245085\n",
      " 0.41584732]\n"
     ]
    }
   ],
   "source": [
    "importances = model.feature_importances_\n",
    "print(importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3cf1c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         importance\n",
      "feature            \n",
      "LSTAT      0.415847\n",
      "RM         0.402682\n",
      "DIS        0.063973\n",
      "CRIM       0.031676\n",
      "PTRATIO    0.018081\n",
      "AGE        0.014299\n",
      "NOX        0.014269\n",
      "B          0.012451\n",
      "TAX        0.011525\n",
      "INDUS      0.007138\n",
      "RAD        0.005281\n",
      "ZN         0.001543\n",
      "CHAS       0.001236\n"
     ]
    }
   ],
   "source": [
    "importances = pd.DataFrame({'feature': X_train.columns,'importance': model.feature_importances_})\n",
    "importances = importances.sort_values('importance',ascending=False).set_index('feature')\n",
    "print(importances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f0742567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuoAAAFlCAYAAAC9ac02AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Z1A+gAAAACXBIWXMAAAsTAAALEwEAmpwYAAAmo0lEQVR4nO3df5idZX3n8feHCRAQiPJDjAEZYfmlBAIMcYtSARXoBsUurBhqS1y9UlTQ+oMa192t7bY1WG1EpdLsqhFWBbWXlkpLRSVVSxUGSIgIooFIE7VKcAMafobv/jEn9DjOTGaSmTnPzHm/rutc8zz3cz/P+Z77Osn5zD33OSdVhSRJkqRm2anTBUiSJEn6dQZ1SZIkqYEM6pIkSVIDGdQlSZKkBjKoS5IkSQ1kUJckSZIaaEanC2iifffdt3p7eztdhiRJkqa5W2655f6q2m+oYwb1IfT29tLf39/pMiRJkjTNJfnhcMdc+iJJkiQ1kEFdkiRJaiCDuiRJktRArlGXJEnqQo8//jjr16/nkUce6XQpXWHmzJkccMAB7LzzzqM+x6A+hDUbNtG75FrWLV3Q6VIkSZImxPr169lzzz3p7e0lSafLmdaqio0bN7J+/Xqe+9znjvo8l75IkiR1oUceeYR99tnHkD4JkrDPPvuM+a8XBnVJkqQuZUifPNsz1o0L6kmeleSqJGuT3JLk75McluQ7g/q9J8k72vZnJPlZkqWD+p2Z5LYkq5N8N8nvT9ZjkSRJ0vBOPPHESb2/devW8elPf3pS73NHNGqNegZ+1fgC8MmqenWr7Rhg/1Gc/jLgbuC/JHlXVVWSnYHlwPyqWp9kV6B3YqqXJEmaunqXXDuu1xvNe/1uvPHGcb3PkTzxxBNPBfXzzjtv0u53RzRtRv0U4PGqunxrQ1WtBv51FOcuBC4F7gN+o9W2JwO/jGxsXevRqvreuFYsSZKk7bLHHnsAsHLlSl784hdz1llncfDBB7NkyRI+9alPMX/+fObOncvatWsBWLRoERdccAF9fX0cdthhfOlLXwIG1tu/9rWvZe7cuRx77LHccMMNAKxYsYJXvOIVnHrqqbzkJS9hyZIlfOMb32DevHksW7aMdevWcdJJJ3Hcccdx3HHHPfWLw8qVKzn55JM555xzOOKII/id3/kdqgqAm2++mRNPPJFjjjmG+fPn89BDD7FlyxYuvvhiTjjhBI4++mj++q//elzGp1Ez6sBRwC3DHDskyaq2/WcB7wdIMhN4KfD7wNMZCO03VtUDSa4Bfpjkq8CXgM9U1ZODL55kMbAYoGev/cblwUiSJGl0Vq9ezZ133snee+/NwQcfzOtf/3puuukmLr30Uj784Q/zwQ9+EBhYvnLTTTexdu1aTjnlFH7wgx9w2WWXkYQ1a9Zw1113cdppp3H33XcDcOutt3L77bez9957s3LlSt7//vc/FfA3b97M9ddfz8yZM/n+97/PwoUL6e/vB+C2227jjjvu4NnPfjYvfOEL+ed//mfmz5/Pueeey9VXX80JJ5zAgw8+yG677cbHPvYxZs2axc0338yjjz7KC1/4Qk477bQxfcLLUJoW1Eeytqrmbd1J8p62Y2cCN1TVw0n+BvgfSf6gqrZU1euTzGUgyL+DgSUyiwZfvKqWM7BMhl1nH1oT9igkSZL0a0444QRmz54NwCGHHMJpp50GwNy5c5+aIQd41atexU477cShhx7KwQcfzF133cU3v/lNLrroIgCOOOIIDjrooKeC+ste9jL23nvvIe/z8ccf58ILL2TVqlX09PQ8dQ7A/PnzOeCAAwCYN28e69atY9asWcyePZsTTjgBgL322guAL3/5y9x+++18/vOfB2DTpk18//vfn3ZB/Q7gnO04byHwoiTrWvv7AKcC1wNU1RpgTZIrgXsZIqhLkiSpc3bdddentnfaaaen9nfaaSeeeOKJp44N/vSUbX2aytOe9rRhjy1btoz999+f1atX8+STTzJz5swh6+np6fmVGgarKj784Q9z+umnj1jLWDVtjfrXgF1by1AASHI0cOBwJyTZCzgJeE5V9VZVL/AmYGGSPZKc3NZ9HvDD8S9bkiRJk+Fzn/scTz75JGvXruWee+7h8MMP56STTuJTn/oUAHfffTf33Xcfhx9++K+du+eee/LQQw89tb9p0yZmz57NTjvtxJVXXsmWLVtGvO/DDz+cH//4x9x8880APPTQQzzxxBOcfvrpfPSjH+Xxxx9/qoZf/vKXO/xYGzWj3vqklt8GPpjkncAjwDrgD0Y47beBr1XVo21tfwu8D3gr8IdJ/hp4GPglzqZLkiRNWc95znOYP38+Dz74IJdffjkzZ87kjW98I294wxuYO3cuM2bMYMWKFb8yI77V0UcfTU9PD8cccwyLFi3ijW98I2effTZXXHEFZ5xxxoiz7wC77LILV199NRdddBEPP/wwu+22G1/5yld4/etfz7p16zjuuOOoKvbbbz+++MUv7vBjzdZ3sOrf7Tr70Jp9/gdH9bFCkiRJU9Gdd97JkUce2ekyxmTRokWceeaZnHPO9qyU7ryhxjzJLVXVN1T/pi19aYS5c2YZ0iVJktRRjVr6IkmSJA1nxYoVnS5hUjmjLkmSJDWQQV2SJKlL+V7FybM9Y21QlyRJ6kIzZ85k48aNhvVJUFVs3LjxVz6nfTRcoy5JktSFDjjgANavX8/PfvazTpfSFWbOnPnUN52OlkFdkiSpC+288847/BX3mlgufZEkSZIayKAuSZIkNZBBXZIkSWogg7okSZLUQAZ1SZIkqYEM6pIkSVIDGdSHsGbDJnqXXEvvkms7XYokSZK6lEFdkiRJaiCDuiRJktRA0yKoJ/ntJKsG3Z5M8oYkleSitr4fSbKog+VKkiRJ2zQtgnpVfaGq5m29AX8FfAP4R+CnwFuS7NLJGiVJkqSxmBZBvV2Sw4D/Cfwu8CTwM+CrwPmdrEuSJEkai2kV1JPsDHwaeHtV3dd26BLgHUl6Rjh3cZL+JP1bNm+a6FIlSZKkEU2roA78L+COqrq6vbGq7gG+DZw33IlVtbyq+qqqr2f3WRNcpiRJkjSyGZ0uYLwkORk4GzhumC5/Dnwe+KdJKkmSJEnabtNiRj3JM4BPAL9XVQ8N1aeq7gK+C7x8MmuTJEmStsd0mVG/AHgm8NEk7e2fGdTvz4DbJqsoSZIkaXtNi6BeVe8F3jvM4Uva+q1mmvwVQZIkSdOboVWSJElqoGkxoz7e5s6ZRf/SBZ0uQ5IkSV3MGXVJkiSpgQzqkiRJUgMZ1CVJkqQGMqhLkiRJDWRQlyRJkhrIoC5JkiQ1kEFdkiRJaiCDuiRJktRABnVJkiSpgQzqkiRJUgMZ1CVJkqQGmtHpAppozYZN9C65dsQ+65YumKRqJEmS1I2cUZckSZIayKAuSZIkNdCUCepJtiRZleQ7Sf4uydMHHV+V5KpBbSuS3JtkdZK7k1yR5IBJLVySJEnaDlMmqAMPV9W8qjoKeAB409YDSY4EeoCTkjxt0HkXV9UxwOHAbcDXkuwyWUVLkiRJ22MqBfV2/wLMadtfCFwJfBk4a6gTasAy4CfAb014hZIkSdIOmHJBPUkP8BLgmrbmc4GrgM8wENpHcitwxBDXXZykP0n/ls2bxqtcSZIkabtMpaC+W5JVDMyI7w9cD5CkD7i/qu4Dvgocm2TvEa6ToRqranlV9VVVX8/us8a3ckmSJGmMplJQf7iq5gEHMRC2t65RXwgckWQdsBbYCzh7hOscC9w5cWVKkiRJO24qBXUAqmoz8Gbg7a03hb4KmFtVvVXVy8Aa9V9b/pIBbwZmA9dNYsmSJEnSmE25oA5QVbcBtwPvAjZU1Y/aDn8deF6S2a39v0iyGrgbOAE4paoem9SCJUmSpDGa0ekCRquq9hi0//LW5h8Pat8CPKu1u2jiK5MkSZLG35ScUZckSZKmuykzoz6Z5s6ZRf/SBZ0uQ5IkSV3MGXVJkiSpgQzqkiRJUgMZ1CVJkqQGMqhLkiRJDWRQlyRJkhrIoC5JkiQ1kEFdkiRJaiCDuiRJktRABnVJkiSpgQzqkiRJUgMZ1CVJkqQGmtHpAppozYZN9C65dlR91y1dMMHVSJIkqRs5oy5JkiQ1kEFdkiRJaqCOB/Ukv2j97E1SSS5qO/aRJIta2yuS3JtkdZK7k1yR5IDB12nbX5TkI63tw5OsTLIqyZ1Jlk/Kg5MkSZK2U8eD+iA/Bd6SZJdhjl9cVccAhwO3AV8boW+7DwHLqmpeVR0JfHh8ypUkSZImRtOC+s+ArwLnj9SpBiwDfgL81iiuOxtY33b+mh0pUpIkSZpoTQvqAJcA70jSM4q+twJHjKLfMgZm3/8hyVuTPH1whySLk/Qn6d+yedPYKpYkSZLGWeOCelXdA3wbOG8U3bOty7Wu+QngSOBzwMnAt5LsOuh+l1dVX1X19ew+a8x1S5IkSeOpcUG95c+Bd7LtIH4scGdr++FB69X3Bu7fulNVP6qqj1fVWcATwFHjWK8kSZI0rhoZ1KvqLuC7wMuHOp4Bb2Zg7fl1reZ/Al7TOr4b8Crghtb+GUl2bm0/C9gH2DCRj0GSJEnaEY0M6i1/BhwwqO0vkqwG7gZOAE6pqsdax94C/Ockq4BvAZ+rqq+3jp0GfKd17j8y8OkxP5noByBJkiRtrxmdLqCq9mj9XEfbcpSqWk3bLxJVtWgb19kAnDnMsbcBb9vxaiVJkqTJ0eQZdUmSJKlrdXxGvYnmzplF/9IFnS5DkiRJXcwZdUmSJKmBDOqSJElSAxnUJUmSpAYyqEuSJEkNZFCXJEmSGsigLkmSJDWQQV2SJElqIIO6JEmS1EAGdUmSJKmBDOqSJElSAxnUJUmSpAaa0ekCmmjNhk30Lrl2h6+zbumCcahGkiRJ3cgZdUmSJKmBDOqSJElSA02poJ5knySrWrefJNnQtv/MJI8nuaCt/55J1iY5tLW/c5I1SV7QuUchSZIkbduUCupVtbGq5lXVPOByYFnb/tnAt4CFbf0fAt4FfKTV9A7gxqr69qQWLkmSJI3RlArq27AQeDswJ8kBWxur6rMASf4QuICB4C5JkiQ12rQI6kkOBGZX1U3AZ4FzB3V5C3AJ8KdV9cBk1ydJkiSN1bQI6gwE88+2tq+ibflLyxnAj4GjhrtAksVJ+pP0b9m8aWKqlCRJkkZpugT1hcCiJOuAa4Cj295A+mzgzcB84D8lOXqoC1TV8qrqq6q+nt1nTVLZkiRJ0tCmfFBPchiwR1XNqarequoF3su/z6ovA/68qtYDbwMuS5LOVCtJkiSNzpQP6gwE8i8MavsbYGGSlwHPAT4GUFV/B/wc+L1JrVCSJEkaoxmdLmB7VdV7Rjh2O3Bka/f6QcdeMYFlSZIkSeNiOsyoS5IkSdPOlJ1Rn0hz58yif+mCTpchSZKkLuaMuiRJktRABnVJkiSpgQzqkiRJUgMZ1CVJkqQGMqhLkiRJDWRQlyRJkhrIoC5JkiQ1kEFdkiRJaiCDuiRJktRABnVJkiSpgQzqkiRJUgPN6HQBTbRmwyZ6l1w7btdbt3TBuF1LkiRJ3cEZdUmSJKmBDOqSJElSA3VFUE+yJcmqJKuT3JrkxE7XJEmSJI2kW9aoP1xV8wCSnA68F3hxRyuSJEmSRtAVM+qD7AX8vNNFSJIkSSPplhn13ZKsAmYCs4FTO1uOJEmSNLJuCertS19+A7giyVFVVVs7JFkMLAbo2Wu/jhQpSZIkbdV1S1+q6l+AfYH9BrUvr6q+qurr2X1WZ4qTJEmSWrouqCc5AugBNna6FkmSJGk43bL0ZesadYAA51fVlg7WI0mSJI2oK4J6VfV0ugZJkiRpLLpu6YskSZI0FXTFjPpYzZ0zi/6lCzpdhiRJkrqYM+qSJElSAxnUJUmSpAbaZlDPgNck+Z+t/eckmT/xpUmSJEndazQz6n8F/AawsLX/EHDZhFUkSZIkaVRvJn1BVR2X5DaAqvp5kl0muC5JkiSpq41mRv3xJD1AASTZD3hyQquSJEmSutxogvqHgC8Az0zyZ8A3gT+f0KokSZKkLjfi0pckOwH3An8IvAQI8MqqunMSapMkSZK61ohBvaqeTHJZVR0L3DVJNUmSJEldbzRLX76a5OwkmfBqJEmSJAGjC+q/D3wOeDTJg0keSvLgBNclSZIkdbVtfjxjVe05GYU0yZoNm+hdcu24X3fd0gXjfk1JkiRNT9sM6kl+c6j2qvr6+JcjSZIkCUb3hUcXt23PBOYDtwCnTkhFkiRJkra9Rr2qXt52exlwFPDziS/tVyWpJB9o239Hkve07S9OclfrdlOSF7Xa35bk4239fifJ+K9rkSRJksbRaN5MOth64MjxLmQUHgX+c5J9Bx9IciYDb3p9UVUdAVwAfDrJsxj4wqbjkrwwydOBPwUumryyJUmSpLEbzRr1DwPV2t0JmAfcOoE1DecJYDnwVuDdg469E7i4qu4HqKpbk3wSeFNV/Y8kbwT+CrgJ+HhV3TOJdUuSJEljNpo16v1t208An6mqf56gerblMuD2JO8b1P58BtbNt+sHzgeoqhuT3Am8lM78NUCSJEkak9EE9adX1aXtDUneMrhtMlTVg0muAN4MPDza85LsAfQBOwP7MbB8Z3CfxcBigJ699huXeiVJkqTtNZo16ucP0bZonOsYiw8CrwOe1tb2XeD4Qf2OB+5obf8x8H+BPwOWDXXRqlpeVX1V1dez+6xxLViSJEkaq2Fn1JMsBM4DnpvkmrZDewIPTHRhw6mqB5J8loGwvvXTXN4HXJLkjKramGQeA79MvCDJXGABA2vrHwNel+RlVXX9pBcvSZIkjdJIS19uBH4M7At8oK39IeD2iSxqFD4AXLh1p6quSTIHuDFJMVDja4CfAJ8D3lpVjwAkeQNwRZJ5VfXY5JcuSZIkbduwQb2qfgj8EPiNyStneFW1R9v2vwG7Dzr+UeCjQ5z6okH9+oHnTUSNkiRJ0njZ5hr1JP8xyc1JfpHksSRbkjw4GcVJkiRJ3Wo0n/ryEeDVDCwh6QN+DzhsIovqtLlzZtG/dEGny5AkSVIXG9U3k1bVD4CeqtpSVZ8AzpjYsiRJkqTuNpoZ9c1JdgFWtb5o6MeMMuBLkiRJ2j6jCdy/2+p3IfBL4EDg7IksSpIkSep225xRr6ofJtkNmF1VfzwJNUmSJEldbzSf+vJyYBVwXWt/3qAvQJIkSZI0zkaz9OU9wHzg/wFU1SrguRNWkSRJkqRRBfXHq2rToLaaiGIkSZIkDRjNp77ckeQ8oCfJocCbgRsntixJkiSpuw07o57kytbmWuD5wKPAZ4AHgT+Y8MokSZKkLjbSjPrxSZ4NnAucAnyg7djuwCMTWZgkSZLUzUYK6pcDXwUOBvrb2sPAGvWDJ7AuSZIkqasNu/Slqj5UVUcCH6+qg9tuz60qQ7okSZI0gVLlB7gMtuvsQ2v2+R8c9+uuW7pg3K8pSZKkqSvJLVXVN9Sx0Xw8oyRJkqRJNuWCepJXJqkkR7S1zU+yMsn3k9ya5Nokc1vH3pNkQ5JVbbend+wBSJIkSaMwms9Rb5qFwDdbP/8oyf7AZ4HzqupGgCQvAg4B1rTOWVZV7+9EsZIkSdL2mFJBPckewIsY+LjIvwP+CLgQ+OTWkA5QVd/sTIWSJEnS+JhqS1/OAq6rqruBjUmOZ+DLmG7dxnlvbVv2csOEVylJkiTtoKkW1BcCV7W2r2rt/4ok305yZ5JL25qXVdW81u2UoS6cZHGS/iT9WzZvGv/KJUmSpDGYMktfkuwNnArMTVJADwNfvPRJ4DjgbwGq6gVJzgHOHMv1q2o5sBwGPp5xHEuXJEmSxmwqzaifA1xZVQdVVW9VHQjcC1wPLEpyYlvf3TtSoSRJkjROpsyMOgPLXC4Z1PY3rfZzgUuSzAF+CtwP/Elbv7cmeU3b/iurat0E1ipJkiTtkCkT1IdaW15VH2rbffEw570HeM/EVCVJkiRNjKm09EWSJEnqGlNmRn0yzZ0zi/6lCzpdhiRJkrqYM+qSJElSAxnUJUmSpAYyqEuSJEkNZFCXJEmSGsigLkmSJDWQQV2SJElqIIO6JEmS1EAGdUmSJKmBDOqSJElSAxnUJUmSpAYyqEuSJEkNZFCXJEmSGmhGpwtoojUbNtG75NpJu791SxdM2n1JkiRpanBGXZIkSWqgCZtRT7IFWNO6jzuBPwC2TlM/C9gC/Ky1Px94uK3/vcDvVtX/a7veKuCuqnp1ktcCb2kdeh7wvdb1rgPuAvqq6sLWeYuBt7X6Pgi8raq+Ob6PVpIkSRpfEzmj/nBVzauqo4DHgHNb+/OAy4FlW/er6rFB/R8A3rT1QkmOBHqAk5I8rao+0XatHwGntPaXtBeQ5Ezg94EXVdURwAXAp5M8awIftyRJkrTDJmvpyzeA/zCG/v8CzGnbXwhcCXwZOGsM13kncHFV3Q9QVbcCn6TtlwBJkiSpiSY8qCeZAfwWA8taRtO/B3gJcE1b87nAVcBnGAjto/V84JZBbf2tdkmSJKmxJjKo79ZaV94P3Ad8bJT9fwLsD1wPkKQPuL+q7gO+ChybZO/xLjbJ4iT9Sfq3bN403peXJEmSxmQy1qjPq6qLWuvQt9kfOAgI/748ZSFwRJJ1wFpgL+DsUdbwXeD4QW3HA3cM7lhVy6uqr6r6enafNcrLS5IkSROjcR/PWFWbgTcDb0+yC/AqYG5V9VZVLwNr1Ee7/OV9wCVJ9gFIMg9YBPzVOJctSZIkjatGfuFRVd2W5HbgXcCGqvpR2+GvA89LMruqfryN61yTZA5wY5ICHgJes63zJEmSpE5LVXW6hsbZdfahNfv8D07a/fnNpJIkSd0pyS1V1TfUscYtfZEkSZLU0KUvnTZ3ziz6neWWJElSBzmjLkmSJDWQQV2SJElqIIO6JEmS1EAGdUmSJKmBDOqSJElSAxnUJUmSpAYyqEuSJEkNZFCXJEmSGsigLkmSJDWQQV2SJElqIIO6JEmS1EAGdUmSJKmBZnS6gCZas2ETvUuu7XQZO2Td0gWdLkGSJEk7wBl1SZIkqYEM6pIkSVIDNTqoJ3lWkquSrE1yS5K/T3JYkoeTrEry3SRXJNm51f/kJF9qbS9KUkle2na9V7bazunUY5IkSZJGo7FBPUmALwArq+qQqjoeeBewP7C2quYBc4EDgFcNc5k1wKvb9hcCqyesaEmSJGmcNDaoA6cAj1fV5Vsbqmo18K9t+1uAm4A5w1zjG8D8JDsn2QP4D8CqCatYkiRJGidNDupHAbeM1CHJTOAFwHXDdCngK8DpwFnANSNca3GS/iT9WzZv2r6KJUmSpHHS5KA+kkOSrAL+DfhxVd0+Qt+rGFj+8mrgM8N1qqrlVdVXVX09u88a12IlSZKksWpyUL8DOH6YY1vXqB8CHJ/kFcNdpKpuYmAt+75Vdfe4VylJkiRNgCYH9a8BuyZZvLUhydHAgVv3q+p+YAkDbzIdyRLgv01EkZIkSdJEaGxQr6oCfht4aevjGe8A3gv8ZFDXLwK7JzlphGv9Q1XdMGHFSpIkSeNsRqcLGElV/YihP3rxqLY+BRzTdmxlq30FsGKIay4axxIlSZKkCdHooN4pc+fMon/pgk6XIUmSpC7W2KUvkiRJUjczqEuSJEkNZFCXJEmSGsigLkmSJDWQQV2SJElqIIO6JEmS1EAGdUmSJKmBDOqSJElSAxnUJUmSpAYyqEuSJEkNZFCXJEmSGsigLkmSJDXQjE4X0ERrNmyid8m1nS5DwLqlCzpdgiRJUkc4oy5JkiQ1kEFdkiRJaqApHdSTbEmyKskdSVYneXuSnVrHTk7ypdb2/km+1Orz3SR/39nKJUmSpJFN9TXqD1fVPIAkzwQ+DewF/NGgfn8CXF9Vl7b6Hj2ZRUqSJEljNaVn1NtV1U+BxcCFSTLo8GxgfVvf2yezNkmSJGmspk1QB6iqe4Ae4JmDDl0GfCzJDUneneTZg89NsjhJf5L+LZs3TUa5kiRJ0rCmVVAfTlX9I3Aw8L+BI4Dbkuw3qM/yquqrqr6e3Wd1okxJkiTpKdMqqCc5GNgC/HTwsap6oKo+XVW/C9wM/OZk1ydJkiSN1rQJ6q0Z8suBj1RVDTp2apLdW9t7AocA901+lZIkSdLoTPVPfdktySpgZ+AJ4ErgL4fodzzwkSRPMPDLyf+pqpsnrUpJkiRpjKZ0UK+qnhGOrQRWtrb/AviLyalKkiRJ2nFTOqhPlLlzZtG/dEGny5AkSVIXmzZr1CVJkqTpxKAuSZIkNZBBXZIkSWogg7okSZLUQAZ1SZIkqYEM6pIkSVIDGdQlSZKkBjKoS5IkSQ1kUJckSZIayKAuSZIkNZBBXZIkSWogg7okSZLUQDM6XUATrdmwid4l13a6DEmSJA2ybumCTpcwaZxRlyRJkhrIoC5JkiQ10JQP6km2JFmV5DtJ/i7J01vtvUkqyZ+29d03yeNJPtKxgiVJkqRRmPJBHXi4quZV1VHAA8Cb2o7dC7QvZPovwB2TWZwkSZK0PaZDUG/3L8Cctv3NwJ1J+lr75wKfnfSqJEmSpDGaNkE9SQ/wEuCaQYeuAl6d5EBgC/CjYc5fnKQ/Sf+WzZsmtlhJkiRpG6ZDUN8tySrgJ8D+wPWDjl8HvAx4NXD1cBepquVV1VdVfT27z5qoWiVJkqRRmQ5B/eGqmgccBIRfXaNOVT0G3AK8Hfj8pFcnSZIkbYfpENQBqKrNwJuBtycZ/EVOHwDeWVUPTH5lkiRJ0thNm6AOUFW3AbcDCwe131FVn+xMVZIkSdLYDZ55nnKqao9B+y9v2z1qiP4rgBUTW5UkSZK0Y6Z8UJ8Ic+fMon/pgm13lCRJkibItFr6IkmSJE0XBnVJkiSpgQzqkiRJUgMZ1CVJkqQGMqhLkiRJDWRQlyRJkhrIoC5JkiQ1kEFdkiRJaiCDuiRJktRABnVJkiSpgQzqkiRJUgMZ1CVJkqQGmtHpAppozYZN9C65ttNlSJIkaYKtW7qg0yUMyxl1SZIkqYEM6pIkSVIDNSaoJ/nFEG2HJ1mZZFWSO5MsT3J6a39Vkl8k+V5r+4rWOa9MUkmOaO1/u3X8viQ/azu3d5IfoiRJkjRqTV+j/iFgWVX9LUCSuVW1BvjH1v5K4B1V1d92zkLgm62ff1RVL2j1XQT0VdWFk1e+JEmStH0aM6M+jNnA+q07rZA+rCR7AC8CXge8emJLkyRJkiZO04P6MuBrSf4hyVuTPH0b/c8Crququ4GNSY4f7R0lWZykP0n/ls2bdqBkSZIkacc1OqhX1SeAI4HPAScD30qy6winLASuam1f1dof7X0tr6q+qurr2X3WdlYsSZIkjY+mr1Gnqn4EfBz4eJLvAEcBtwzul2Rv4FRgbpICeoBKcnFV1WTWLEmSJO2oRs+oJzkjyc6t7WcB+wAbhul+DnBlVR1UVb1VdSBwL3DS5FQrSZIkjZ8mzajvnmR92/5fAgcAlyZ5pNV2cVX9ZJjzFwKXDGr7m1b718e1UkmSJGmCNSaoV9Vws/tvG+Gck9u2Txni+IfatlcAK7a7QEmSJGkSNXrpiyRJktStGjOj3iRz58yif+mCTpchSZKkLuaMuiRJktRABnVJkiSpgQzqkiRJUgMZ1CVJkqQGMqhLkiRJDZSq6nQNjZPkIeB7na5jitkXuL/TRUwxjtnYOF5j55iNnWM2do7Z2DheYzfdx+ygqtpvqAN+POPQvldVfZ0uYipJ0u+YjY1jNjaO19g5ZmPnmI2dYzY2jtfYdfOYufRFkiRJaiCDuiRJktRABvWhLe90AVOQYzZ2jtnYOF5j55iNnWM2do7Z2DheY9e1Y+abSSVJkqQGckZdkiRJaqCuC+pJzkjyvSQ/SLJkiOO7Jrm6dfzbSXrbjr2r1f69JKdPauEdtL1jlqQ3ycNJVrVul0968R0wivH6zSS3JnkiyTmDjp2f5Put2/mTV3Vn7eCYbWl7jl0zeVV31ijG7G1Jvpvk9iRfTXJQ27Gue57t4Hj5HBt6zC5IsqY1Lt9M8ry2Y75ejmHMfL0cerza+p2dpJL0tbV1x3OsqrrmBvQAa4GDgV2A1cDzBvV5I3B5a/vVwNWt7ee1+u8KPLd1nZ5OP6aGj1kv8J1OP4YGjlcvcDRwBXBOW/vewD2tn89obT+j04+pyWPWOvaLTj+Gho7ZKcDure03tP277Lrn2Y6Ml8+xEcdsr7btVwDXtbZ9vRz7mPl6OcR4tfrtCXwd+BbQ123PsW6bUZ8P/KCq7qmqx4CrgLMG9TkL+GRr+/PAS5Kk1X5VVT1aVfcCP2hdb7rbkTHrRtscr6paV1W3A08OOvd04PqqeqCqfg5cD5wxGUV32I6MWbcazZjdUFWbW7vfAg5obXfj82xHxqtbjWbMHmzbfRqw9U1vvl6Ofcy60WjyBcD/Ai4BHmlr65rnWLcF9TnAv7btr2+1Ddmnqp4ANgH7jPLc6WhHxgzguUluS/JPSU6a6GIbYEeeJz7HBoz1cc9M0p/kW0leOa6VNddYx+x1wD9s57nTwY6MF/gcg2HGLMmbkqwF3ge8eSznTkM7Mmbg6+WvjVeS44ADq+rasZ47XfjNpJpIPwaeU1UbkxwPfDHJ8wfNKEg76qCq2pDkYOBrSdZU1dpOF9UUSV4D9AEv7nQtU8Ew4+VzbBhVdRlwWZLzgP8OdMV7HnbEMGPm6+UgSXYC/hJY1OFSOqrbZtQ3AAe27R/QahuyT5IZwCxg4yjPnY62e8xaf5LaCFBVtzCwhuywCa+4s3bkeeJzbMCYHndVbWj9vAdYCRw7nsU11KjGLMlLgXcDr6iqR8dy7jSzI+Plc2zAtp4nVwGv3M5zp4vtHjNfL4FfH689gaOAlUnWAf8RuKb1htLueY51epH8ZN4Y+AvCPQy88WDrGxeeP6jPm/jVN0Z+trX9fH71jQv3ME3fuDCOY7bf1jFi4M0iG4C9O/2YOj1ebX1X8OtvJr2XgTf4PaO1Pa3HaxzG7BnArq3tfYHvM8SbkabbbZT/Lo9l4MX+0EHtXfc828Hx8jk2/Jgd2rb9cqC/te3r5djHzNfLEf7vb/Vfyb+/mbRrnmMdL6ADT4z/BNzd+g/53a22P2FgBgVgJvA5Bt6YcBNwcNu5726d9z3gtzr9WJo+ZsDZwB3AKuBW4OWdfiwNGa8TGFhP90sG/lpzR9u5/7U1jj8AXtvpx9L0MQNOBNa0/sNeA7yu04+lQWP2FeDfWv/+VgHXdPPzbHvHy+fYiGN2adv/8TfQFrJ8vRzbmPl6OfR4Deq7klZQ76bnmN9MKkmSJDVQt61RlyRJkqYEg7okSZLUQAZ1SZIkqYEM6pIkSVIDGdQlSZKkBjKoS5IkSQ1kUJckSZIayKAuSZIkNdD/B5ec4KmLUMo8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from pylab import rcParams\n",
    "rcParams[\"figure.figsize\"] = 12, 6\n",
    "importances.plot(kind=\"barh\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7bc90f50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(boston[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabd190c",
   "metadata": {},
   "source": [
    " Важнейшие признаки: LSTAT - статус населения, RM - среднее количество комнат в одном жилом помещении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7e2d05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0184b295",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3c486332",
   "metadata": {},
   "source": [
    "*Задание 4\n",
    "В этом задании мы будем работать с датасетом, с которым мы уже знакомы по домашнему заданию по библиотеке Matplotlib, это датасет Credit Card Fraud Detection.Для этого датасета мы будем решать задачу классификации - будем определять,какие из транзакциции по кредитной карте являются мошенническими.Данный датасет сильно несбалансирован (так как случаи мошенничества относительно редки),так что применение метрики accuracy не принесет пользы и не поможет выбрать лучшую модель.Мы будем вычислять AUC, то есть площадь под кривой ROC.\n",
    "Импортируйте из соответствующих модулей RandomForestClassifier, GridSearchCV и train_test_split.\n",
    "Загрузите датасет creditcard.csv и создайте датафрейм df.\n",
    "С помощью метода value_counts с аргументом normalize=True убедитесь в том, что выборка несбалансирована.Используя метод info, проверьте, все ли столбцы содержат числовые данные и нет ли в них пропусков.Примените следующую настройку, чтобы можно было просматривать все столбцы датафрейма:\n",
    "pd.options.display.max_columns = 100.\n",
    "Просмотрите первые 10 строк датафрейма df.\n",
    "Создайте датафрейм X из датафрейма df, исключив столбец Class.\n",
    "Создайте объект Series под названием y из столбца Class.\n",
    "Разбейте X и y на тренировочный и тестовый наборы данных при помощи функции train_test_split, используя аргументы: test_size=0.3, random_state=100, stratify=y.\n",
    "У вас должны получиться объекты X_train, X_test, y_train и y_test.\n",
    "Просмотрите информацию о их форме.\n",
    "Для поиска по сетке параметров задайте такие параметры:\n",
    "parameters = [{'n_estimators': [10, 15],\n",
    "'max_features': np.arange(3, 5),\n",
    "'max_depth': np.arange(4, 7)}]\n",
    "Создайте модель GridSearchCV со следующими аргументами:\n",
    "estimator=RandomForestClassifier(random_state=100),\n",
    "param_grid=parameters,\n",
    "scoring='roc_auc',\n",
    "cv=3.\n",
    "Обучите модель на тренировочном наборе данных (может занять несколько минут).\n",
    "Просмотрите параметры лучшей модели с помощью атрибута best_params_.\n",
    "Предскажите вероятности классов с помощью полученнной модели и метода predict_proba.\n",
    "Из полученного результата (массив Numpy) выберите столбец с индексом 1 (вероятность класса 1) и запишите в массив y_pred_proba. Из модуля sklearn.metrics импортируйте метрику roc_auc_score.\n",
    "Вычислите AUC на тестовых данных и сравните с результатом,полученным на тренировочных данных, используя в качестве аргументов массивы y_test и y_pred_proba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e722deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a2698e68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('D:\\Data_science\\PythonDC\\урок3\\creditcard.csv', delimiter=',')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "37c79c05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.00      0.048061\n",
       "1.98      0.021221\n",
       "0.89      0.017106\n",
       "9.99      0.016667\n",
       "15.00     0.011517\n",
       "            ...   \n",
       "438.10    0.000004\n",
       "152.29    0.000004\n",
       "156.45    0.000004\n",
       "724.44    0.000004\n",
       "102.77    0.000004\n",
       "Name: Amount, Length: 32767, dtype: float64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Amount'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "30606c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0b8eb5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8f80d289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>-0.551600</td>\n",
       "      <td>-0.617801</td>\n",
       "      <td>-0.991390</td>\n",
       "      <td>-0.311169</td>\n",
       "      <td>1.468177</td>\n",
       "      <td>-0.470401</td>\n",
       "      <td>0.207971</td>\n",
       "      <td>0.025791</td>\n",
       "      <td>0.403993</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>1.612727</td>\n",
       "      <td>1.065235</td>\n",
       "      <td>0.489095</td>\n",
       "      <td>-0.143772</td>\n",
       "      <td>0.635558</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>-0.114805</td>\n",
       "      <td>-0.183361</td>\n",
       "      <td>-0.145783</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>0.624501</td>\n",
       "      <td>0.066084</td>\n",
       "      <td>0.717293</td>\n",
       "      <td>-0.165946</td>\n",
       "      <td>2.345865</td>\n",
       "      <td>-2.890083</td>\n",
       "      <td>1.109969</td>\n",
       "      <td>-0.121359</td>\n",
       "      <td>-2.261857</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>-0.226487</td>\n",
       "      <td>0.178228</td>\n",
       "      <td>0.507757</td>\n",
       "      <td>-0.287924</td>\n",
       "      <td>-0.631418</td>\n",
       "      <td>-1.059647</td>\n",
       "      <td>-0.684093</td>\n",
       "      <td>1.965775</td>\n",
       "      <td>-1.232622</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>-0.822843</td>\n",
       "      <td>0.538196</td>\n",
       "      <td>1.345852</td>\n",
       "      <td>-1.119670</td>\n",
       "      <td>0.175121</td>\n",
       "      <td>-0.451449</td>\n",
       "      <td>-0.237033</td>\n",
       "      <td>-0.038195</td>\n",
       "      <td>0.803487</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-0.425966</td>\n",
       "      <td>0.960523</td>\n",
       "      <td>1.141109</td>\n",
       "      <td>-0.168252</td>\n",
       "      <td>0.420987</td>\n",
       "      <td>-0.029728</td>\n",
       "      <td>0.476201</td>\n",
       "      <td>0.260314</td>\n",
       "      <td>-0.568671</td>\n",
       "      <td>-0.371407</td>\n",
       "      <td>1.341262</td>\n",
       "      <td>0.359894</td>\n",
       "      <td>-0.358091</td>\n",
       "      <td>-0.137134</td>\n",
       "      <td>0.517617</td>\n",
       "      <td>0.401726</td>\n",
       "      <td>-0.058133</td>\n",
       "      <td>0.068653</td>\n",
       "      <td>-0.033194</td>\n",
       "      <td>0.084968</td>\n",
       "      <td>-0.208254</td>\n",
       "      <td>-0.559825</td>\n",
       "      <td>-0.026398</td>\n",
       "      <td>-0.371427</td>\n",
       "      <td>-0.232794</td>\n",
       "      <td>0.105915</td>\n",
       "      <td>0.253844</td>\n",
       "      <td>0.081080</td>\n",
       "      <td>3.67</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.229658</td>\n",
       "      <td>0.141004</td>\n",
       "      <td>0.045371</td>\n",
       "      <td>1.202613</td>\n",
       "      <td>0.191881</td>\n",
       "      <td>0.272708</td>\n",
       "      <td>-0.005159</td>\n",
       "      <td>0.081213</td>\n",
       "      <td>0.464960</td>\n",
       "      <td>-0.099254</td>\n",
       "      <td>-1.416907</td>\n",
       "      <td>-0.153826</td>\n",
       "      <td>-0.751063</td>\n",
       "      <td>0.167372</td>\n",
       "      <td>0.050144</td>\n",
       "      <td>-0.443587</td>\n",
       "      <td>0.002821</td>\n",
       "      <td>-0.611987</td>\n",
       "      <td>-0.045575</td>\n",
       "      <td>-0.219633</td>\n",
       "      <td>-0.167716</td>\n",
       "      <td>-0.270710</td>\n",
       "      <td>-0.154104</td>\n",
       "      <td>-0.780055</td>\n",
       "      <td>0.750137</td>\n",
       "      <td>-0.257237</td>\n",
       "      <td>0.034507</td>\n",
       "      <td>0.005168</td>\n",
       "      <td>4.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.644269</td>\n",
       "      <td>1.417964</td>\n",
       "      <td>1.074380</td>\n",
       "      <td>-0.492199</td>\n",
       "      <td>0.948934</td>\n",
       "      <td>0.428118</td>\n",
       "      <td>1.120631</td>\n",
       "      <td>-3.807864</td>\n",
       "      <td>0.615375</td>\n",
       "      <td>1.249376</td>\n",
       "      <td>-0.619468</td>\n",
       "      <td>0.291474</td>\n",
       "      <td>1.757964</td>\n",
       "      <td>-1.323865</td>\n",
       "      <td>0.686133</td>\n",
       "      <td>-0.076127</td>\n",
       "      <td>-1.222127</td>\n",
       "      <td>-0.358222</td>\n",
       "      <td>0.324505</td>\n",
       "      <td>-0.156742</td>\n",
       "      <td>1.943465</td>\n",
       "      <td>-1.015455</td>\n",
       "      <td>0.057504</td>\n",
       "      <td>-0.649709</td>\n",
       "      <td>-0.415267</td>\n",
       "      <td>-0.051634</td>\n",
       "      <td>-1.206921</td>\n",
       "      <td>-1.085339</td>\n",
       "      <td>40.80</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7.0</td>\n",
       "      <td>-0.894286</td>\n",
       "      <td>0.286157</td>\n",
       "      <td>-0.113192</td>\n",
       "      <td>-0.271526</td>\n",
       "      <td>2.669599</td>\n",
       "      <td>3.721818</td>\n",
       "      <td>0.370145</td>\n",
       "      <td>0.851084</td>\n",
       "      <td>-0.392048</td>\n",
       "      <td>-0.410430</td>\n",
       "      <td>-0.705117</td>\n",
       "      <td>-0.110452</td>\n",
       "      <td>-0.286254</td>\n",
       "      <td>0.074355</td>\n",
       "      <td>-0.328783</td>\n",
       "      <td>-0.210077</td>\n",
       "      <td>-0.499768</td>\n",
       "      <td>0.118765</td>\n",
       "      <td>0.570328</td>\n",
       "      <td>0.052736</td>\n",
       "      <td>-0.073425</td>\n",
       "      <td>-0.268092</td>\n",
       "      <td>-0.204233</td>\n",
       "      <td>1.011592</td>\n",
       "      <td>0.373205</td>\n",
       "      <td>-0.384157</td>\n",
       "      <td>0.011747</td>\n",
       "      <td>0.142404</td>\n",
       "      <td>93.20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.0</td>\n",
       "      <td>-0.338262</td>\n",
       "      <td>1.119593</td>\n",
       "      <td>1.044367</td>\n",
       "      <td>-0.222187</td>\n",
       "      <td>0.499361</td>\n",
       "      <td>-0.246761</td>\n",
       "      <td>0.651583</td>\n",
       "      <td>0.069539</td>\n",
       "      <td>-0.736727</td>\n",
       "      <td>-0.366846</td>\n",
       "      <td>1.017614</td>\n",
       "      <td>0.836390</td>\n",
       "      <td>1.006844</td>\n",
       "      <td>-0.443523</td>\n",
       "      <td>0.150219</td>\n",
       "      <td>0.739453</td>\n",
       "      <td>-0.540980</td>\n",
       "      <td>0.476677</td>\n",
       "      <td>0.451773</td>\n",
       "      <td>0.203711</td>\n",
       "      <td>-0.246914</td>\n",
       "      <td>-0.633753</td>\n",
       "      <td>-0.120794</td>\n",
       "      <td>-0.385050</td>\n",
       "      <td>-0.069733</td>\n",
       "      <td>0.094199</td>\n",
       "      <td>0.246219</td>\n",
       "      <td>0.083076</td>\n",
       "      <td>3.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "5   2.0 -0.425966  0.960523  1.141109 -0.168252  0.420987 -0.029728  0.476201   \n",
       "6   4.0  1.229658  0.141004  0.045371  1.202613  0.191881  0.272708 -0.005159   \n",
       "7   7.0 -0.644269  1.417964  1.074380 -0.492199  0.948934  0.428118  1.120631   \n",
       "8   7.0 -0.894286  0.286157 -0.113192 -0.271526  2.669599  3.721818  0.370145   \n",
       "9   9.0 -0.338262  1.119593  1.044367 -0.222187  0.499361 -0.246761  0.651583   \n",
       "\n",
       "         V8        V9       V10       V11       V12       V13       V14  \\\n",
       "0  0.098698  0.363787  0.090794 -0.551600 -0.617801 -0.991390 -0.311169   \n",
       "1  0.085102 -0.255425 -0.166974  1.612727  1.065235  0.489095 -0.143772   \n",
       "2  0.247676 -1.514654  0.207643  0.624501  0.066084  0.717293 -0.165946   \n",
       "3  0.377436 -1.387024 -0.054952 -0.226487  0.178228  0.507757 -0.287924   \n",
       "4 -0.270533  0.817739  0.753074 -0.822843  0.538196  1.345852 -1.119670   \n",
       "5  0.260314 -0.568671 -0.371407  1.341262  0.359894 -0.358091 -0.137134   \n",
       "6  0.081213  0.464960 -0.099254 -1.416907 -0.153826 -0.751063  0.167372   \n",
       "7 -3.807864  0.615375  1.249376 -0.619468  0.291474  1.757964 -1.323865   \n",
       "8  0.851084 -0.392048 -0.410430 -0.705117 -0.110452 -0.286254  0.074355   \n",
       "9  0.069539 -0.736727 -0.366846  1.017614  0.836390  1.006844 -0.443523   \n",
       "\n",
       "        V15       V16       V17       V18       V19       V20       V21  \\\n",
       "0  1.468177 -0.470401  0.207971  0.025791  0.403993  0.251412 -0.018307   \n",
       "1  0.635558  0.463917 -0.114805 -0.183361 -0.145783 -0.069083 -0.225775   \n",
       "2  2.345865 -2.890083  1.109969 -0.121359 -2.261857  0.524980  0.247998   \n",
       "3 -0.631418 -1.059647 -0.684093  1.965775 -1.232622 -0.208038 -0.108300   \n",
       "4  0.175121 -0.451449 -0.237033 -0.038195  0.803487  0.408542 -0.009431   \n",
       "5  0.517617  0.401726 -0.058133  0.068653 -0.033194  0.084968 -0.208254   \n",
       "6  0.050144 -0.443587  0.002821 -0.611987 -0.045575 -0.219633 -0.167716   \n",
       "7  0.686133 -0.076127 -1.222127 -0.358222  0.324505 -0.156742  1.943465   \n",
       "8 -0.328783 -0.210077 -0.499768  0.118765  0.570328  0.052736 -0.073425   \n",
       "9  0.150219  0.739453 -0.540980  0.476677  0.451773  0.203711 -0.246914   \n",
       "\n",
       "        V22       V23       V24       V25       V26       V27       V28  \\\n",
       "0  0.277838 -0.110474  0.066928  0.128539 -0.189115  0.133558 -0.021053   \n",
       "1 -0.638672  0.101288 -0.339846  0.167170  0.125895 -0.008983  0.014724   \n",
       "2  0.771679  0.909412 -0.689281 -0.327642 -0.139097 -0.055353 -0.059752   \n",
       "3  0.005274 -0.190321 -1.175575  0.647376 -0.221929  0.062723  0.061458   \n",
       "4  0.798278 -0.137458  0.141267 -0.206010  0.502292  0.219422  0.215153   \n",
       "5 -0.559825 -0.026398 -0.371427 -0.232794  0.105915  0.253844  0.081080   \n",
       "6 -0.270710 -0.154104 -0.780055  0.750137 -0.257237  0.034507  0.005168   \n",
       "7 -1.015455  0.057504 -0.649709 -0.415267 -0.051634 -1.206921 -1.085339   \n",
       "8 -0.268092 -0.204233  1.011592  0.373205 -0.384157  0.011747  0.142404   \n",
       "9 -0.633753 -0.120794 -0.385050 -0.069733  0.094199  0.246219  0.083076   \n",
       "\n",
       "   Amount  Class  \n",
       "0  149.62      0  \n",
       "1    2.69      0  \n",
       "2  378.66      0  \n",
       "3  123.50      0  \n",
       "4   69.99      0  \n",
       "5    3.67      0  \n",
       "6    4.99      0  \n",
       "7   40.80      0  \n",
       "8   93.20      0  \n",
       "9    3.68      0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "695fb9cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>V12</th>\n",
       "      <th>V13</th>\n",
       "      <th>V14</th>\n",
       "      <th>V15</th>\n",
       "      <th>V16</th>\n",
       "      <th>V17</th>\n",
       "      <th>V18</th>\n",
       "      <th>V19</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>0.090794</td>\n",
       "      <td>-0.551600</td>\n",
       "      <td>-0.617801</td>\n",
       "      <td>-0.991390</td>\n",
       "      <td>-0.311169</td>\n",
       "      <td>1.468177</td>\n",
       "      <td>-0.470401</td>\n",
       "      <td>0.207971</td>\n",
       "      <td>0.025791</td>\n",
       "      <td>0.403993</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>-0.166974</td>\n",
       "      <td>1.612727</td>\n",
       "      <td>1.065235</td>\n",
       "      <td>0.489095</td>\n",
       "      <td>-0.143772</td>\n",
       "      <td>0.635558</td>\n",
       "      <td>0.463917</td>\n",
       "      <td>-0.114805</td>\n",
       "      <td>-0.183361</td>\n",
       "      <td>-0.145783</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>0.207643</td>\n",
       "      <td>0.624501</td>\n",
       "      <td>0.066084</td>\n",
       "      <td>0.717293</td>\n",
       "      <td>-0.165946</td>\n",
       "      <td>2.345865</td>\n",
       "      <td>-2.890083</td>\n",
       "      <td>1.109969</td>\n",
       "      <td>-0.121359</td>\n",
       "      <td>-2.261857</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>-0.054952</td>\n",
       "      <td>-0.226487</td>\n",
       "      <td>0.178228</td>\n",
       "      <td>0.507757</td>\n",
       "      <td>-0.287924</td>\n",
       "      <td>-0.631418</td>\n",
       "      <td>-1.059647</td>\n",
       "      <td>-0.684093</td>\n",
       "      <td>1.965775</td>\n",
       "      <td>-1.232622</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>0.753074</td>\n",
       "      <td>-0.822843</td>\n",
       "      <td>0.538196</td>\n",
       "      <td>1.345852</td>\n",
       "      <td>-1.119670</td>\n",
       "      <td>0.175121</td>\n",
       "      <td>-0.451449</td>\n",
       "      <td>-0.237033</td>\n",
       "      <td>-0.038195</td>\n",
       "      <td>0.803487</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284802</th>\n",
       "      <td>172786.0</td>\n",
       "      <td>-11.881118</td>\n",
       "      <td>10.071785</td>\n",
       "      <td>-9.834783</td>\n",
       "      <td>-2.066656</td>\n",
       "      <td>-5.364473</td>\n",
       "      <td>-2.606837</td>\n",
       "      <td>-4.918215</td>\n",
       "      <td>7.305334</td>\n",
       "      <td>1.914428</td>\n",
       "      <td>4.356170</td>\n",
       "      <td>-1.593105</td>\n",
       "      <td>2.711941</td>\n",
       "      <td>-0.689256</td>\n",
       "      <td>4.626942</td>\n",
       "      <td>-0.924459</td>\n",
       "      <td>1.107641</td>\n",
       "      <td>1.991691</td>\n",
       "      <td>0.510632</td>\n",
       "      <td>-0.682920</td>\n",
       "      <td>1.475829</td>\n",
       "      <td>0.213454</td>\n",
       "      <td>0.111864</td>\n",
       "      <td>1.014480</td>\n",
       "      <td>-0.509348</td>\n",
       "      <td>1.436807</td>\n",
       "      <td>0.250034</td>\n",
       "      <td>0.943651</td>\n",
       "      <td>0.823731</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284803</th>\n",
       "      <td>172787.0</td>\n",
       "      <td>-0.732789</td>\n",
       "      <td>-0.055080</td>\n",
       "      <td>2.035030</td>\n",
       "      <td>-0.738589</td>\n",
       "      <td>0.868229</td>\n",
       "      <td>1.058415</td>\n",
       "      <td>0.024330</td>\n",
       "      <td>0.294869</td>\n",
       "      <td>0.584800</td>\n",
       "      <td>-0.975926</td>\n",
       "      <td>-0.150189</td>\n",
       "      <td>0.915802</td>\n",
       "      <td>1.214756</td>\n",
       "      <td>-0.675143</td>\n",
       "      <td>1.164931</td>\n",
       "      <td>-0.711757</td>\n",
       "      <td>-0.025693</td>\n",
       "      <td>-1.221179</td>\n",
       "      <td>-1.545556</td>\n",
       "      <td>0.059616</td>\n",
       "      <td>0.214205</td>\n",
       "      <td>0.924384</td>\n",
       "      <td>0.012463</td>\n",
       "      <td>-1.016226</td>\n",
       "      <td>-0.606624</td>\n",
       "      <td>-0.395255</td>\n",
       "      <td>0.068472</td>\n",
       "      <td>-0.053527</td>\n",
       "      <td>24.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284804</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>1.919565</td>\n",
       "      <td>-0.301254</td>\n",
       "      <td>-3.249640</td>\n",
       "      <td>-0.557828</td>\n",
       "      <td>2.630515</td>\n",
       "      <td>3.031260</td>\n",
       "      <td>-0.296827</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.432454</td>\n",
       "      <td>-0.484782</td>\n",
       "      <td>0.411614</td>\n",
       "      <td>0.063119</td>\n",
       "      <td>-0.183699</td>\n",
       "      <td>-0.510602</td>\n",
       "      <td>1.329284</td>\n",
       "      <td>0.140716</td>\n",
       "      <td>0.313502</td>\n",
       "      <td>0.395652</td>\n",
       "      <td>-0.577252</td>\n",
       "      <td>0.001396</td>\n",
       "      <td>0.232045</td>\n",
       "      <td>0.578229</td>\n",
       "      <td>-0.037501</td>\n",
       "      <td>0.640134</td>\n",
       "      <td>0.265745</td>\n",
       "      <td>-0.087371</td>\n",
       "      <td>0.004455</td>\n",
       "      <td>-0.026561</td>\n",
       "      <td>67.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284805</th>\n",
       "      <td>172788.0</td>\n",
       "      <td>-0.240440</td>\n",
       "      <td>0.530483</td>\n",
       "      <td>0.702510</td>\n",
       "      <td>0.689799</td>\n",
       "      <td>-0.377961</td>\n",
       "      <td>0.623708</td>\n",
       "      <td>-0.686180</td>\n",
       "      <td>0.679145</td>\n",
       "      <td>0.392087</td>\n",
       "      <td>-0.399126</td>\n",
       "      <td>-1.933849</td>\n",
       "      <td>-0.962886</td>\n",
       "      <td>-1.042082</td>\n",
       "      <td>0.449624</td>\n",
       "      <td>1.962563</td>\n",
       "      <td>-0.608577</td>\n",
       "      <td>0.509928</td>\n",
       "      <td>1.113981</td>\n",
       "      <td>2.897849</td>\n",
       "      <td>0.127434</td>\n",
       "      <td>0.265245</td>\n",
       "      <td>0.800049</td>\n",
       "      <td>-0.163298</td>\n",
       "      <td>0.123205</td>\n",
       "      <td>-0.569159</td>\n",
       "      <td>0.546668</td>\n",
       "      <td>0.108821</td>\n",
       "      <td>0.104533</td>\n",
       "      <td>10.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284806</th>\n",
       "      <td>172792.0</td>\n",
       "      <td>-0.533413</td>\n",
       "      <td>-0.189733</td>\n",
       "      <td>0.703337</td>\n",
       "      <td>-0.506271</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.649617</td>\n",
       "      <td>1.577006</td>\n",
       "      <td>-0.414650</td>\n",
       "      <td>0.486180</td>\n",
       "      <td>-0.915427</td>\n",
       "      <td>-1.040458</td>\n",
       "      <td>-0.031513</td>\n",
       "      <td>-0.188093</td>\n",
       "      <td>-0.084316</td>\n",
       "      <td>0.041333</td>\n",
       "      <td>-0.302620</td>\n",
       "      <td>-0.660377</td>\n",
       "      <td>0.167430</td>\n",
       "      <td>-0.256117</td>\n",
       "      <td>0.382948</td>\n",
       "      <td>0.261057</td>\n",
       "      <td>0.643078</td>\n",
       "      <td>0.376777</td>\n",
       "      <td>0.008797</td>\n",
       "      <td>-0.473649</td>\n",
       "      <td>-0.818267</td>\n",
       "      <td>-0.002415</td>\n",
       "      <td>0.013649</td>\n",
       "      <td>217.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>284807 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Time         V1         V2        V3        V4        V5  \\\n",
       "0            0.0  -1.359807  -0.072781  2.536347  1.378155 -0.338321   \n",
       "1            0.0   1.191857   0.266151  0.166480  0.448154  0.060018   \n",
       "2            1.0  -1.358354  -1.340163  1.773209  0.379780 -0.503198   \n",
       "3            1.0  -0.966272  -0.185226  1.792993 -0.863291 -0.010309   \n",
       "4            2.0  -1.158233   0.877737  1.548718  0.403034 -0.407193   \n",
       "...          ...        ...        ...       ...       ...       ...   \n",
       "284802  172786.0 -11.881118  10.071785 -9.834783 -2.066656 -5.364473   \n",
       "284803  172787.0  -0.732789  -0.055080  2.035030 -0.738589  0.868229   \n",
       "284804  172788.0   1.919565  -0.301254 -3.249640 -0.557828  2.630515   \n",
       "284805  172788.0  -0.240440   0.530483  0.702510  0.689799 -0.377961   \n",
       "284806  172792.0  -0.533413  -0.189733  0.703337 -0.506271 -0.012546   \n",
       "\n",
       "              V6        V7        V8        V9       V10       V11       V12  \\\n",
       "0       0.462388  0.239599  0.098698  0.363787  0.090794 -0.551600 -0.617801   \n",
       "1      -0.082361 -0.078803  0.085102 -0.255425 -0.166974  1.612727  1.065235   \n",
       "2       1.800499  0.791461  0.247676 -1.514654  0.207643  0.624501  0.066084   \n",
       "3       1.247203  0.237609  0.377436 -1.387024 -0.054952 -0.226487  0.178228   \n",
       "4       0.095921  0.592941 -0.270533  0.817739  0.753074 -0.822843  0.538196   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "284802 -2.606837 -4.918215  7.305334  1.914428  4.356170 -1.593105  2.711941   \n",
       "284803  1.058415  0.024330  0.294869  0.584800 -0.975926 -0.150189  0.915802   \n",
       "284804  3.031260 -0.296827  0.708417  0.432454 -0.484782  0.411614  0.063119   \n",
       "284805  0.623708 -0.686180  0.679145  0.392087 -0.399126 -1.933849 -0.962886   \n",
       "284806 -0.649617  1.577006 -0.414650  0.486180 -0.915427 -1.040458 -0.031513   \n",
       "\n",
       "             V13       V14       V15       V16       V17       V18       V19  \\\n",
       "0      -0.991390 -0.311169  1.468177 -0.470401  0.207971  0.025791  0.403993   \n",
       "1       0.489095 -0.143772  0.635558  0.463917 -0.114805 -0.183361 -0.145783   \n",
       "2       0.717293 -0.165946  2.345865 -2.890083  1.109969 -0.121359 -2.261857   \n",
       "3       0.507757 -0.287924 -0.631418 -1.059647 -0.684093  1.965775 -1.232622   \n",
       "4       1.345852 -1.119670  0.175121 -0.451449 -0.237033 -0.038195  0.803487   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "284802 -0.689256  4.626942 -0.924459  1.107641  1.991691  0.510632 -0.682920   \n",
       "284803  1.214756 -0.675143  1.164931 -0.711757 -0.025693 -1.221179 -1.545556   \n",
       "284804 -0.183699 -0.510602  1.329284  0.140716  0.313502  0.395652 -0.577252   \n",
       "284805 -1.042082  0.449624  1.962563 -0.608577  0.509928  1.113981  2.897849   \n",
       "284806 -0.188093 -0.084316  0.041333 -0.302620 -0.660377  0.167430 -0.256117   \n",
       "\n",
       "             V20       V21       V22       V23       V24       V25       V26  \\\n",
       "0       0.251412 -0.018307  0.277838 -0.110474  0.066928  0.128539 -0.189115   \n",
       "1      -0.069083 -0.225775 -0.638672  0.101288 -0.339846  0.167170  0.125895   \n",
       "2       0.524980  0.247998  0.771679  0.909412 -0.689281 -0.327642 -0.139097   \n",
       "3      -0.208038 -0.108300  0.005274 -0.190321 -1.175575  0.647376 -0.221929   \n",
       "4       0.408542 -0.009431  0.798278 -0.137458  0.141267 -0.206010  0.502292   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "284802  1.475829  0.213454  0.111864  1.014480 -0.509348  1.436807  0.250034   \n",
       "284803  0.059616  0.214205  0.924384  0.012463 -1.016226 -0.606624 -0.395255   \n",
       "284804  0.001396  0.232045  0.578229 -0.037501  0.640134  0.265745 -0.087371   \n",
       "284805  0.127434  0.265245  0.800049 -0.163298  0.123205 -0.569159  0.546668   \n",
       "284806  0.382948  0.261057  0.643078  0.376777  0.008797 -0.473649 -0.818267   \n",
       "\n",
       "             V27       V28  Amount  \n",
       "0       0.133558 -0.021053  149.62  \n",
       "1      -0.008983  0.014724    2.69  \n",
       "2      -0.055353 -0.059752  378.66  \n",
       "3       0.062723  0.061458  123.50  \n",
       "4       0.219422  0.215153   69.99  \n",
       "...          ...       ...     ...  \n",
       "284802  0.943651  0.823731    0.77  \n",
       "284803  0.068472 -0.053527   24.79  \n",
       "284804  0.004455 -0.026561   67.88  \n",
       "284805  0.108821  0.104533   10.00  \n",
       "284806 -0.002415  0.013649  217.00  \n",
       "\n",
       "[284807 rows x 30 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df.drop(['Class'], axis=1)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ccaba3d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = df['Class'].tolist()\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "25810e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2174b6cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(199364, 30)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "21debaef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85443, 30)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0a33631e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "aed7ebf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a6232c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [{'n_estimators': [10, 15],\n",
    "'max_features': np.arange(3, 5),\n",
    "'max_depth': np.arange(4, 7)}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "edb5276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=100),\n",
    "    param_grid=parameters,\n",
    "    scoring='roc_auc',\n",
    "    cv=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "11738fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=RandomForestClassifier(random_state=100),\n",
       "             param_grid=[{'max_depth': array([4, 5, 6]),\n",
       "                          'max_features': array([3, 4]),\n",
       "                          'n_estimators': [10, 15]}],\n",
       "             scoring='roc_auc')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5fc45fbf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 6, 'max_features': 4, 'n_estimators': 15}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "50c9aadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(max_depth=6, max_features=4, n_estimators=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6950c349",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "88ac0504",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = classifier.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1935a80b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00085759, 0.00027782, 0.00027952, ..., 0.00027782, 0.00027782,\n",
       "       0.00035716])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_proba = predictions[:,1]\n",
    "y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2a1945ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "eaa0d17b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.956\n"
     ]
    }
   ],
   "source": [
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "print('AUC: %.3f' % auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e4dd1a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a85f8c54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.plot(fpr, tpr, marker='.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75a2bee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
